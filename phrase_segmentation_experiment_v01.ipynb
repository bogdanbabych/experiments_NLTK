{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/DYUUB0SFin8MnuiKfdRJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bogdanbabych/experiments_NLTK/blob/main/phrase_segmentation_experiment_v01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIQgvGkWbdEz",
        "outputId": "54d41cb0-ea8a-4966-f982-7b1e2ac23302"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing sentence: \"The quick brown fox jumps over the lazy dog.\"\n",
            "+---------------------+-------------+------------------------+---------------------+----------------------------------------------------------------------------------+\n",
            "| Segment             | Type        | Words (Original)       | POS Tags            | Dependency Annotation                                                            |\n",
            "+=====================+=============+========================+=====================+==================================================================================+\n",
            "| The quick brown fox | Noun Phrase | The, quick, brown, fox | DET, ADJ, ADJ, NOUN | The -> fox (det); quick -> fox (amod); brown -> fox (amod); fox -> jumps (nsubj) |\n",
            "+---------------------+-------------+------------------------+---------------------+----------------------------------------------------------------------------------+\n",
            "| jumps               | VERB        | jumps                  | VERB                | jumps -> ROOT (ROOT)                                                             |\n",
            "+---------------------+-------------+------------------------+---------------------+----------------------------------------------------------------------------------+\n",
            "| over                | ADP         | over                   | ADP                 | over -> jumps (prep)                                                             |\n",
            "+---------------------+-------------+------------------------+---------------------+----------------------------------------------------------------------------------+\n",
            "| the lazy dog        | Noun Phrase | the, lazy, dog         | DET, ADJ, NOUN      | the -> dog (det); lazy -> dog (amod); dog -> over (pobj)                         |\n",
            "+---------------------+-------------+------------------------+---------------------+----------------------------------------------------------------------------------+\n",
            "| .                   | PUNCT       | .                      | PUNCT               | . -> jumps (punct)                                                               |\n",
            "+---------------------+-------------+------------------------+---------------------+----------------------------------------------------------------------------------+\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "Analyzing sentence: \"I saw a girl with a telescope.\"\n",
            "+-------------+-------------+--------------------+------------+------------------------------------------------+\n",
            "| Segment     | Type        | Words (Original)   | POS Tags   | Dependency Annotation                          |\n",
            "+=============+=============+====================+============+================================================+\n",
            "| I           | Noun Phrase | I                  | PRON       | I -> saw (nsubj)                               |\n",
            "+-------------+-------------+--------------------+------------+------------------------------------------------+\n",
            "| saw         | VERB        | saw                | VERB       | saw -> ROOT (ROOT)                             |\n",
            "+-------------+-------------+--------------------+------------+------------------------------------------------+\n",
            "| a girl      | Noun Phrase | a, girl            | DET, NOUN  | a -> girl (det); girl -> saw (dobj)            |\n",
            "+-------------+-------------+--------------------+------------+------------------------------------------------+\n",
            "| with        | ADP         | with               | ADP        | with -> girl (prep)                            |\n",
            "+-------------+-------------+--------------------+------------+------------------------------------------------+\n",
            "| a telescope | Noun Phrase | a, telescope       | DET, NOUN  | a -> telescope (det); telescope -> with (pobj) |\n",
            "+-------------+-------------+--------------------+------------+------------------------------------------------+\n",
            "| .           | PUNCT       | .                  | PUNCT      | . -> saw (punct)                               |\n",
            "+-------------+-------------+--------------------+------------+------------------------------------------------+\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "Analyzing sentence: \"Artificial intelligence is transforming many industries around the globe.\"\n",
            "+-------------------------+-------------+--------------------------+------------+-------------------------------------------------------------------------+\n",
            "| Segment                 | Type        | Words (Original)         | POS Tags   | Dependency Annotation                                                   |\n",
            "+=========================+=============+==========================+============+=========================================================================+\n",
            "| Artificial intelligence | Noun Phrase | Artificial, intelligence | ADJ, NOUN  | Artificial -> intelligence (amod); intelligence -> transforming (nsubj) |\n",
            "+-------------------------+-------------+--------------------------+------------+-------------------------------------------------------------------------+\n",
            "| is                      | AUX         | is                       | AUX        | is -> transforming (aux)                                                |\n",
            "+-------------------------+-------------+--------------------------+------------+-------------------------------------------------------------------------+\n",
            "| transforming            | VERB        | transforming             | VERB       | transforming -> ROOT (ROOT)                                             |\n",
            "+-------------------------+-------------+--------------------------+------------+-------------------------------------------------------------------------+\n",
            "| many industries         | Noun Phrase | many, industries         | ADJ, NOUN  | many -> industries (amod); industries -> transforming (dobj)            |\n",
            "+-------------------------+-------------+--------------------------+------------+-------------------------------------------------------------------------+\n",
            "| around                  | ADP         | around                   | ADP        | around -> industries (prep)                                             |\n",
            "+-------------------------+-------------+--------------------------+------------+-------------------------------------------------------------------------+\n",
            "| the globe               | Noun Phrase | the, globe               | DET, NOUN  | the -> globe (det); globe -> around (pobj)                              |\n",
            "+-------------------------+-------------+--------------------------+------------+-------------------------------------------------------------------------+\n",
            "| .                       | PUNCT       | .                        | PUNCT      | . -> transforming (punct)                                               |\n",
            "+-------------------------+-------------+--------------------------+------------+-------------------------------------------------------------------------+\n",
            "\n",
            "====================================================================================================\n",
            "\n",
            "Analyzing sentence: \"She quickly ran to the store and bought some fresh apples.\"\n",
            "+-------------------+-------------+---------------------+----------------+-----------------------------------------------------------------------+\n",
            "| Segment           | Type        | Words (Original)    | POS Tags       | Dependency Annotation                                                 |\n",
            "+===================+=============+=====================+================+=======================================================================+\n",
            "| She               | Noun Phrase | She                 | PRON           | She -> ran (nsubj)                                                    |\n",
            "+-------------------+-------------+---------------------+----------------+-----------------------------------------------------------------------+\n",
            "| quickly           | ADV         | quickly             | ADV            | quickly -> ran (advmod)                                               |\n",
            "+-------------------+-------------+---------------------+----------------+-----------------------------------------------------------------------+\n",
            "| ran               | VERB        | ran                 | VERB           | ran -> ROOT (ROOT)                                                    |\n",
            "+-------------------+-------------+---------------------+----------------+-----------------------------------------------------------------------+\n",
            "| to                | ADP         | to                  | ADP            | to -> ran (prep)                                                      |\n",
            "+-------------------+-------------+---------------------+----------------+-----------------------------------------------------------------------+\n",
            "| the store         | Noun Phrase | the, store          | DET, NOUN      | the -> store (det); store -> to (pobj)                                |\n",
            "+-------------------+-------------+---------------------+----------------+-----------------------------------------------------------------------+\n",
            "| and               | CCONJ       | and                 | CCONJ          | and -> ran (cc)                                                       |\n",
            "+-------------------+-------------+---------------------+----------------+-----------------------------------------------------------------------+\n",
            "| bought            | VERB        | bought              | VERB           | bought -> ran (conj)                                                  |\n",
            "+-------------------+-------------+---------------------+----------------+-----------------------------------------------------------------------+\n",
            "| some fresh apples | Noun Phrase | some, fresh, apples | DET, ADJ, NOUN | some -> apples (det); fresh -> apples (amod); apples -> bought (dobj) |\n",
            "+-------------------+-------------+---------------------+----------------+-----------------------------------------------------------------------+\n",
            "| .                 | PUNCT       | .                   | PUNCT          | . -> ran (punct)                                                      |\n",
            "+-------------------+-------------+---------------------+----------------+-----------------------------------------------------------------------+\n",
            "\n",
            "====================================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from tabulate import tabulate # Used for generating nice table output\n",
        "\n",
        "# Load English tokenizer, tagger, parser, and NER.\n",
        "# This block attempts to load the model and downloads it if not found.\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    # If the model is not found, download it. This requires internet access.\n",
        "    spacy.cli.download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def segment_sentence_to_phrases_and_dependencies(sentence: str) -> str:\n",
        "    \"\"\"\n",
        "    Performs segmentation of a sentence into phrases (primarily noun phrases\n",
        "    and individual word segments), along with their Part-of-Speech (POS)\n",
        "    tags and dependency structure annotations. The analysis is presented\n",
        "    in a structured table format.\n",
        "\n",
        "    The function first identifies noun phrases using spaCy's built-in\n",
        "    noun chunker. Then, it iterates through all remaining tokens that\n",
        "    were not part of a recognized noun phrase, treating them as individual\n",
        "    segments. For each segment, it extracts:\n",
        "    - The segment text\n",
        "    - Its type (e.g., 'Noun Phrase', or the POS tag for single words)\n",
        "    - The original words within the segment\n",
        "    - Their respective Part-of-Speech tags\n",
        "    - A detailed annotation of their dependency relationships within the sentence.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence as a string to be analyzed.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the formatted table with the segmentation\n",
        "             results, POS tags, and dependency annotations.\n",
        "    \"\"\"\n",
        "    # Process the input sentence using the loaded spaCy NLP pipeline\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # This list will store dictionaries, each representing a segment to be\n",
        "    # displayed in the table. We store 'start_index' to allow sorting later\n",
        "    # to maintain the original sentence order in the table.\n",
        "    processed_segments = []\n",
        "\n",
        "    # A set to keep track of indices of tokens that have already been covered\n",
        "    # by a noun chunk, to avoid re-processing them as individual words.\n",
        "    covered_tokens_indices = set()\n",
        "\n",
        "    # --- Step 1: Process Noun Phrases ---\n",
        "    # Iterate through all detected noun chunks in the document.\n",
        "    for chunk in doc.noun_chunks:\n",
        "        # Extract the text of words within the current noun chunk\n",
        "        segment_words = [token.text for token in chunk]\n",
        "        # Extract the POS tags for each word in the noun chunk\n",
        "        segment_pos = [token.pos_ for token in chunk]\n",
        "\n",
        "        # Prepare dependency annotations for each token within the chunk.\n",
        "        # This shows the token, its head, and the dependency type.\n",
        "        dependency_annotations = []\n",
        "        for token in chunk:\n",
        "            # If a token is its own head, it's typically the root of the sentence\n",
        "            head_text = token.head.text if token.head != token else \"SELF\"\n",
        "            dep_type = token.dep_\n",
        "            dependency_annotations.append(f\"{token.text} -> {head_text} ({dep_type})\")\n",
        "            # Mark the token's index as covered\n",
        "            covered_tokens_indices.add(token.i)\n",
        "\n",
        "        # Add the processed noun chunk's data to our list of segments.\n",
        "        processed_segments.append({\n",
        "            'start_index': chunk.start_char, # Starting character index for sorting\n",
        "            'data_row': [\n",
        "                chunk.text,                   # The full text of the noun chunk\n",
        "                \"Noun Phrase\",                # Type of segment\n",
        "                \", \".join(segment_words),     # Comma-separated original words\n",
        "                \", \".join(segment_pos),       # Comma-separated POS tags\n",
        "                \"; \".join(dependency_annotations) # Semicolon-separated dependency annotations\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    # --- Step 2: Process Individual Tokens (not part of noun chunks) ---\n",
        "    # Iterate through all tokens in the document.\n",
        "    for token in doc:\n",
        "        # If the token's index has not been covered by a noun chunk, process it.\n",
        "        if token.i not in covered_tokens_indices:\n",
        "            segment_text = token.text\n",
        "            segment_type = token.pos_ # For single words, use their POS as the type\n",
        "            segment_words = token.text # The word itself\n",
        "            segment_pos = token.pos_   # The word's POS tag\n",
        "\n",
        "            # Get dependency information for this single token.\n",
        "            head_text = token.head.text if token.head != token else \"ROOT\" # \"ROOT\" for the main verb\n",
        "            dep_type = token.dep_\n",
        "            dependency_annotations = f\"{token.text} -> {head_text} ({dep_type})\"\n",
        "\n",
        "            # Add the processed individual token's data to our list of segments.\n",
        "            processed_segments.append({\n",
        "                'start_index': token.idx, # Starting character index for sorting\n",
        "                'data_row': [\n",
        "                    segment_text,\n",
        "                    segment_type,\n",
        "                    segment_words,\n",
        "                    segment_pos,\n",
        "                    dependency_annotations\n",
        "                ]\n",
        "            })\n",
        "\n",
        "    # --- Step 3: Sort and Format Output ---\n",
        "    # Sort all collected segments by their starting character index to ensure\n",
        "    # they appear in the table in the order they appear in the original sentence.\n",
        "    processed_segments.sort(key=lambda x: x['start_index'])\n",
        "\n",
        "    # Extract only the data rows for the tabulate function.\n",
        "    final_table_data = [item['data_row'] for item in processed_segments]\n",
        "\n",
        "    # Define the headers for the table.\n",
        "    headers = [\"Segment\", \"Type\", \"Words (Original)\", \"POS Tags\", \"Dependency Annotation\"]\n",
        "\n",
        "    # Use the tabulate library to format the data into a clean grid table.\n",
        "    return tabulate(final_table_data, headers=headers, tablefmt=\"grid\")\n",
        "\n",
        "# Example Usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Test cases to demonstrate the function's capabilities.\n",
        "    sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "    sentence2 = \"I saw a girl with a telescope.\"\n",
        "    sentence3 = \"Artificial intelligence is transforming many industries around the globe.\"\n",
        "    sentence4 = \"She quickly ran to the store and bought some fresh apples.\"\n",
        "\n",
        "    print(\"Analyzing sentence: \\\"\" + sentence1 + \"\\\"\")\n",
        "    print(segment_sentence_to_phrases_and_dependencies(sentence1))\n",
        "    print(\"\\n\" + \"=\"*100 + \"\\n\") # Separator for clarity\n",
        "\n",
        "    print(\"Analyzing sentence: \\\"\" + sentence2 + \"\\\"\")\n",
        "    print(segment_sentence_to_phrases_and_dependencies(sentence2))\n",
        "    print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
        "\n",
        "    print(\"Analyzing sentence: \\\"\" + sentence3 + \"\\\"\")\n",
        "    print(segment_sentence_to_phrases_and_dependencies(sentence3))\n",
        "    print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
        "\n",
        "    print(\"Analyzing sentence: \\\"\" + sentence4 + \"\\\"\")\n",
        "    print(segment_sentence_to_phrases_and_dependencies(sentence4))\n",
        "    print(\"\\n\" + \"=\"*100 + \"\\n\")\n"
      ]
    }
  ]
}