{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bogdanbabych/experiments_NLTK/blob/main/Sentence_Phrase_Segmentation_and_Dependency_Annotation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from tabulate import tabulate # Used for generating nice table output\n",
        "\n",
        "# Load English tokenizer, tagger, parser, and NER.\n",
        "# This block attempts to load the model and downloads it if not found.\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    # If the model is not found, download it. This requires internet access.\n",
        "    spacy.cli.download(\"en_core_web_sm\")\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def segment_sentence_to_phrases_and_dependencies(sentence: str) -> str:\n",
        "    \"\"\"\n",
        "    Performs segmentation of a sentence into phrases (primarily noun phrases\n",
        "    and individual word segments), along with their Part-of-Speech (POS)\n",
        "    tags and dependency structure annotations. The analysis is presented\n",
        "    in a structured table format.\n",
        "\n",
        "    The function first identifies noun phrases using spaCy's built-in\n",
        "    noun chunker. Then, it iterates through all remaining tokens that\n",
        "    were not part of a recognized noun phrase, treating them as individual\n",
        "    segments. For each segment, it extracts:\n",
        "    - The segment text\n",
        "    - Its type (e.g., 'Noun Phrase', or the POS tag for single words)\n",
        "    - The original words within the segment\n",
        "    - Their respective Part-of-Speech tags\n",
        "    - A detailed annotation of their dependency relationships within the sentence.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence as a string to be analyzed.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the formatted table with the segmentation\n",
        "             results, POS tags, and dependency annotations.\n",
        "    \"\"\"\n",
        "    # Process the input sentence using the loaded spaCy NLP pipeline\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    # This list will store dictionaries, each representing a segment to be\n",
        "    # displayed in the table. We store 'start_index' to allow sorting later\n",
        "    # to maintain the original sentence order in the table.\n",
        "    processed_segments = []\n",
        "\n",
        "    # A set to keep track of indices of tokens that have already been covered\n",
        "    # by a noun chunk, to avoid re-processing them as individual words.\n",
        "    covered_tokens_indices = set()\n",
        "\n",
        "    # --- Step 1: Process Noun Phrases ---\n",
        "    # Iterate through all detected noun chunks in the document.\n",
        "    for chunk in doc.noun_chunks:\n",
        "        # Extract the text of words within the current noun chunk\n",
        "        segment_words = [token.text for token in chunk]\n",
        "        # Extract the POS tags for each word in the noun chunk\n",
        "        segment_pos = [token.pos_ for token in chunk]\n",
        "\n",
        "        # Prepare dependency annotations for each token within the chunk.\n",
        "        # This shows the token, its head, and the dependency type.\n",
        "        dependency_annotations = []\n",
        "        for token in chunk:\n",
        "            # If a token is its own head, it's typically the root of the sentence\n",
        "            head_text = token.head.text if token.head != token else \"SELF\"\n",
        "            dep_type = token.dep_\n",
        "            dependency_annotations.append(f\"{token.text} -> {head_text} ({dep_type})\")\n",
        "            # Mark the token's index as covered\n",
        "            covered_tokens_indices.add(token.i)\n",
        "\n",
        "        # Add the processed noun chunk's data to our list of segments.\n",
        "        processed_segments.append({\n",
        "            'start_index': chunk.start_char, # Starting character index for sorting\n",
        "            'data_row': [\n",
        "                chunk.text,                   # The full text of the noun chunk\n",
        "                \"Noun Phrase\",                # Type of segment\n",
        "                \", \".join(segment_words),     # Comma-separated original words\n",
        "                \", \".join(segment_pos),       # Comma-separated POS tags\n",
        "                \"; \".join(dependency_annotations) # Semicolon-separated dependency annotations\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    # --- Step 2: Process Individual Tokens (not part of noun chunks) ---\n",
        "    # Iterate through all tokens in the document.\n",
        "    for token in doc:\n",
        "        # If the token's index has not been covered by a noun chunk, process it.\n",
        "        if token.i not in covered_tokens_indices:\n",
        "            segment_text = token.text\n",
        "            segment_type = token.pos_ # For single words, use their POS as the type\n",
        "            segment_words = token.text # The word itself\n",
        "            segment_pos = token.pos_   # The word's POS tag\n",
        "\n",
        "            # Get dependency information for this single token.\n",
        "            head_text = token.head.text if token.head != token else \"ROOT\" # \"ROOT\" for the main verb\n",
        "            dep_type = token.dep_\n",
        "            dependency_annotations = f\"{token.text} -> {head_text} ({dep_type})\"\n",
        "\n",
        "            # Add the processed individual token's data to our list of segments.\n",
        "            processed_segments.append({\n",
        "                'start_index': token.idx, # Starting character index for sorting\n",
        "                'data_row': [\n",
        "                    segment_text,\n",
        "                    segment_type,\n",
        "                    segment_words,\n",
        "                    segment_pos,\n",
        "                    dependency_annotations\n",
        "                ]\n",
        "            })\n",
        "\n",
        "    # --- Step 3: Sort and Format Output ---\n",
        "    # Sort all collected segments by their starting character index to ensure\n",
        "    # they appear in the table in the order they appear in the original sentence.\n",
        "    processed_segments.sort(key=lambda x: x['start_index'])\n",
        "\n",
        "    # Extract only the data rows for the tabulate function.\n",
        "    final_table_data = [item['data_row'] for item in processed_segments]\n",
        "\n",
        "    # Define the headers for the table.\n",
        "    headers = [\"Segment\", \"Type\", \"Words (Original)\", \"POS Tags\", \"Dependency Annotation\"]\n",
        "\n",
        "    # Use the tabulate library to format the data into a clean grid table.\n",
        "    return tabulate(final_table_data, headers=headers, tablefmt=\"grid\")\n",
        "\n",
        "\n",
        "def segment_sentence_to_clauses_and_dependencies(sentence: str) -> str:\n",
        "    \"\"\"\n",
        "    Segments a sentence into clauses based on dependency parsing,\n",
        "    identifying main verbs as clause heads and grouping their associated\n",
        "    words (subject, objects, and core modifiers). The output is presented\n",
        "    in a structured table format.\n",
        "\n",
        "    This function identifies clauses by:\n",
        "    1. Finding potential clause heads (verbs and auxiliaries with specific\n",
        "       dependency types like ROOT, advcl, ccomp, xcomp, acl, relcl, csubj).\n",
        "    2. For each clause head, it collects its 'core' tokens, which include\n",
        "       the verb itself, its direct subjects (nsubj, csubj), and its direct\n",
        "       objects/complements (dobj, pobj, attr, acomp, opbj).\n",
        "    3. The function attempts to avoid overlapping tokens by marking them\n",
        "       as 'assigned' once they are included in a clause.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence as a string to be analyzed.\n",
        "\n",
        "    Returns:\n",
        "        str: A string containing the formatted table with the clause\n",
        "             segmentation results, POS tags, and dependency annotations.\n",
        "    \"\"\"\n",
        "    doc = nlp(sentence)\n",
        "    clauses_data = []\n",
        "    # Keep track of indices of tokens already assigned to a clause to avoid duplication\n",
        "    assigned_token_indices = set()\n",
        "\n",
        "    def collect_core_clause_tokens(head_token):\n",
        "        \"\"\"\n",
        "        Collects core tokens for a clause given its head token (verb).\n",
        "        Includes the head, its subject(s), object(s), and immediate adverbial/negation modifiers.\n",
        "        \"\"\"\n",
        "        clause_tokens = {head_token}\n",
        "\n",
        "        # Add subject and its direct modifiers\n",
        "        for child in head_token.children:\n",
        "            if child.dep_ in (\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\"):\n",
        "                clause_tokens.add(child)\n",
        "                for subj_mod in child.children:\n",
        "                    if subj_mod.dep_ in (\"amod\", \"det\", \"compound\", \"nummod\", \"quantmod\", \"poss\"):\n",
        "                        clause_tokens.add(subj_mod)\n",
        "            # Add objects/complements and their direct modifiers\n",
        "            elif child.dep_ in (\"dobj\", \"pobj\", \"attr\", \"acomp\", \"opbj\"):\n",
        "                clause_tokens.add(child)\n",
        "                for obj_mod in child.children:\n",
        "                    if obj_mod.dep_ in (\"amod\", \"det\", \"compound\", \"nummod\", \"quantmod\", \"poss\", \"prep\"):\n",
        "                        clause_tokens.add(obj_mod)\n",
        "            # Add direct verb modifiers (adverbs, negation)\n",
        "            elif child.dep_ in (\"advmod\", \"neg\", \"prt\"):\n",
        "                clause_tokens.add(child)\n",
        "            # Add coordination (e.g., 'and' in \"He ran and she jumped.\")\n",
        "            elif child.dep_ in (\"cc\", \"conj\") and (child.pos_ == \"CCONJ\" or child.pos_ == \"VERB\"):\n",
        "                clause_tokens.add(child)\n",
        "\n",
        "        # Sort tokens by their index to form the clause text in original order\n",
        "        return sorted(list(clause_tokens), key=lambda t: t.i)\n",
        "\n",
        "    # Prioritize processing the ROOT clause first\n",
        "    root_clause_head = None\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"ROOT\" and (token.pos_ == \"VERB\" or token.pos_ == \"AUX\"):\n",
        "            root_clause_head = token\n",
        "            break\n",
        "\n",
        "    if root_clause_head:\n",
        "        main_clause_tokens = collect_core_clause_tokens(root_clause_head)\n",
        "\n",
        "        # Mark all tokens in the main clause as assigned\n",
        "        for token in main_clause_tokens:\n",
        "            assigned_token_indices.add(token.i)\n",
        "\n",
        "        main_clause_dep_annotations = []\n",
        "        for token in main_clause_tokens:\n",
        "            head_text = token.head.text if token.head != token else \"ROOT\"\n",
        "            dep_type = token.dep_\n",
        "            main_clause_dep_annotations.append(f\"{token.text} -> {head_text} ({dep_type})\")\n",
        "\n",
        "        clauses_data.append({\n",
        "            'start_index': main_clause_tokens[0].idx if main_clause_tokens else 0,\n",
        "            'data_row': [\n",
        "                \" \".join([t.text for t in main_clause_tokens]),\n",
        "                \"Main Clause\",\n",
        "                \", \".join([t.text for t in main_clause_tokens]),\n",
        "                \", \".join([t.pos_ for t in main_clause_tokens]),\n",
        "                \"; \".join(main_clause_dep_annotations)\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    # Process other potential clause heads (subordinate clauses)\n",
        "    for token in doc:\n",
        "        # A token is a potential subordinate clause head if it's a verb/auxiliary,\n",
        "        # has a dependency relation indicating a subordinate clause, and hasn't\n",
        "        # been assigned to a previous clause (e.g., the main clause).\n",
        "        if (token.pos_ == \"VERB\" or token.pos_ == \"AUX\") and \\\n",
        "           token.dep_ in (\"advcl\", \"ccomp\", \"xcomp\", \"acl\", \"relcl\", \"csubj\", \"conj\") and \\\n",
        "           token.i not in assigned_token_indices:\n",
        "\n",
        "            sub_clause_tokens = collect_core_clause_tokens(token)\n",
        "\n",
        "            # Filter out tokens that were already assigned to earlier clauses (e.g., the main clause)\n",
        "            # This ensures that each token ideally belongs to only one clause segment.\n",
        "            sub_clause_tokens = [t for t in sub_clause_tokens if t.i not in assigned_token_indices]\n",
        "\n",
        "            if sub_clause_tokens:\n",
        "                # Mark these newly found tokens as assigned\n",
        "                for t in sub_clause_tokens:\n",
        "                    assigned_token_indices.add(t.i)\n",
        "\n",
        "                sub_clause_dep_annotations = []\n",
        "                for sub_token in sub_clause_tokens:\n",
        "                    head_text = sub_token.head.text if sub_token.head != sub_token else \"SELF\"\n",
        "                    dep_type = sub_token.dep_\n",
        "                    sub_clause_dep_annotations.append(f\"{sub_token.text} -> {head_text} ({dep_type})\")\n",
        "\n",
        "                clauses_data.append({\n",
        "                    'start_index': sub_clause_tokens[0].idx,\n",
        "                    'data_row': [\n",
        "                        \" \".join([t.text for t in sub_clause_tokens]),\n",
        "                        f\"Subordinate Clause ({token.dep_})\",\n",
        "                        \", \".join([t.text for t in sub_clause_tokens]),\n",
        "                        \", \".join([t.pos_ for t in sub_clause_tokens]),\n",
        "                        \"; \".join(sub_clause_dep_annotations)\n",
        "                    ]\n",
        "                })\n",
        "\n",
        "    # Sort clauses by their starting index to maintain the original sentence order\n",
        "    clauses_data.sort(key=lambda x: x['start_index'])\n",
        "\n",
        "    final_table_data = [item['data_row'] for item in clauses_data]\n",
        "    headers = [\"Clause Text\", \"Type\", \"Words (Original)\", \"POS Tags\", \"Dependency Annotation\"]\n",
        "\n",
        "    return tabulate(final_table_data, headers=headers, tablefmt=\"grid\")\n",
        "\n",
        "# Example Usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Test cases to demonstrate the function's capabilities.\n",
        "    sentence1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "    sentence2 = \"I saw a girl with a telescope.\"\n",
        "    sentence3 = \"Artificial intelligence is transforming many industries around the globe.\"\n",
        "    sentence4 = \"She quickly ran to the store and bought some fresh apples.\"\n",
        "    sentence5 = \"He believed that she would come, but she never did.\"\n",
        "    sentence6 = \"Running quickly, the dog chased the ball that was thrown by the boy.\"\n",
        "\n",
        "\n",
        "    print(\"Analyzing sentence: \\\"\" + sentence1 + \"\\\" (Phrase Segmentation)\")\n",
        "    print(segment_sentence_to_phrases_and_dependencies(sentence1))\n",
        "    print(\"\\n\" + \"=\"*100 + \"\\n\") # Separator for clarity\n",
        "\n",
        "    print(\"Analyzing sentence: \\\"\" + sentence2 + \"\\\" (Phrase Segmentation)\")\n",
        "    print(segment_sentence_to_phrases_and_dependencies(sentence2))\n",
        "    print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
        "\n",
        "    print(\"Analyzing sentence: \\\"\" + sentence3 + \"\\\" (Phrase Segmentation)\")\n",
        "    print(segment_sentence_to_phrases_and_dependencies(sentence3))\n",
        "    print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
        "\n",
        "    print(\"Analyzing sentence: \\\"\" + sentence4 + \"\\\" (Phrase Segmentation)\")\n",
        "    print(segment_sentence_to_phrases_and_dependencies(sentence4))\n",
        "    print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
        "\n",
        "    print(\"Analyzing sentence: \\\"\" + sentence5 + \"\\\" (Clause Segmentation)\")\n",
        "    print(segment_sentence_to_clauses_and_dependencies(sentence5))\n",
        "    print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
        "\n",
        "    print(\"Analyzing sentence: \\\"\" + sentence6 + \"\\\" (Clause Segmentation)\")\n",
        "    print(segment_sentence_to_clauses_and_dependencies(sentence6))\n",
        "    print(\"\\n\" + \"=\"*100 + \"\\n\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "U_101LGpBBaP"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}